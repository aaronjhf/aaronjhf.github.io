<head>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">
  <meta name="author" content="Aaron Hillman">
  <link rel="stylesheet" href="/css/academic.min.4731b10c3a8aac85ab261b8ce101e025.css">
  
    <!-- 1) MathJax v3 config -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$','$'], ['\\(','\\)']],
          displayMath: [['$$','$$'], ['\\[','\\]']]
        }
      };
    </script>
  
    <!-- 2) MathJax v3 loader -->
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
      async>
    </script>
  
    <title>Power Law Spectral Densities and Natural Data | Aaron Hillman</title>
  </head>



  <body>
    <!-- copy over your <nav>…</nav> from index.html -->
  
    <div class="universal-wrapper pt-3">
      <article>
        <header class="article-header">
          <h1 class="article-title">Power Law Spectral Densities and Natural Data</h1>
          <div class="article-metadata">
            <span class="article-date">April 29, 2025</span>
          </div>
        </header>
  
        <div class="article-style">
          <h2>Background</h2>

          <p> It has been observed that the rank-ordered eigenvalues of the covariance matrices of natural datasets exhibit a power-law falloff. This is upstream of the scaling laws that have been observed for the loss curves of models versus model size and training set size.
            In this post I numerically calculate the spectral densities for power-law data and use the implementation to study natural data more closely.
            The case of Gaussian input data, yielding the famous Marchenko-Pastur law for its eigenvalue distribution, already provides the qualitative picture of double-descent and its mitigation by regularization, but does not exhibit power-law behavior and is not sensible 
            to compare directly to natural data or feature-mapped natural data.  For this reason, I develop a numerical implementation of the Silverstein equation, which I use to compute the theoretical sample spectral densities in the large 
            dimension limit, at finite overparameterization. Then I compare to natural image and text datasets where we see vastly improved fits to non-linearly mapped data, as well as interesting structure to the deviations from theory. In particular, I observe log-periodic oscillatory
            behavior to the fractional empirical excesses of the sample eigenvalue distributions.
          </p>

          <!-- Side-by-side figs of e.g. MNIST power-law and with ReLU extending -->
  
          <p>It is useful to recall the matrix resolvent, the $S$-transform, and their relationship to the spectral density that I want to compute.  The matrix resolvent of an $N \times N$ matrix $\mathbf{A}$ is:</p>

          $$ \mathbf{G}_{\mathbf{A}}  = (z \mathbf{I}-\mathbf{A})^{-1}$$
          
          <p> The $S$-transform of the matrix $A$ is a scalar function obtained by taking the normalized trace </p>

          $$g_{\mathbf{A}}(z) = \frac{1}{N} \text{Tr}[\mathbf{G}_{\mathbf{A}}(z)] = \frac{1}{N}\sum\limits_k \frac{1}{\lambda_k-z}$$
          
          <p> This sum over the eigenvalues can of course be written as an integral over the spectral density:</p>

          $$g(z) = \int\limits_{-\infty}^\infty \frac{\rho(z)}{\lambda-z}d\lambda $$

          <p>This equation and its inverse are crucial as there exists a self-consistent equation for $g(z)$ which we can then leverage to determine the spectral density.
            The inversion of the above is:
          </p>

          $$\lim_{\epsilon \to 0}\text{Im}[g(z+i\epsilon)] = \pi \rho(z) $$

          The matrices we want to apply this to are sample covariance matrices for input data with design matrix $X^{\alpha, i} \in \mathbb{R}^{P \times D}$ drawn from a Gaussian with population covariance

          $$\Sigma_P = \text{diag}(1, \dots, \lambda_k, \dots \lambda_D) $$

          <p>with $\lambda_k \sim k^{-\alpha}$ for some non-negative exponent $\alpha$.  The classic Marchenko-Pastur case corresponds to $\alpha = 0$.  We want to compute the $S$-transform of the sample covariance
          </p>
          $$ \Sigma_S = \frac{X^T X}{P} $$
          <p>in the limit as $P, D \to \infty$ with $q = D/P$ held fixed. When this sample covariance is built from Gaussian input vectors, it is called a Wishart matrix. A white Wishart 
            samples every component from the same Gaussian. A structured Gaussian has non-trivial structure to the eigenvalues of the population covariance. The theoretical distribution of sample covariance eigenvalues in the large dimension limit is what we will compare to the empirical
            sample covariance eigenvalue distributions for natural data, such as CIFAR-10, MNIST, and Wikitext.
          </p>

          <p> The final ingredient needed is the most non-trivial: the self-consistent equation for $g(z)$. This is the Silverstein equation: </p>

          $$ g(z) = \int \frac{1}{t \Delta +z}dH(t) $$

          <p>where $\Delta$ is</p>
          
          $$\Delta = -1+q-qzg(z) $$

          <p> and $dH(t)$ is the density of the eigenvalues of the population covariance matrix, $dH(t) \sim t^{-1-1/\alpha}$  for $\alpha > 0$ and $\delta(t-1)$ when $\alpha =0$.
            Indeed we recover the usual equation for $g(z)$ for the $\alpha = 0$ case
          </p>
          $$ \frac{1}{g(z)} = z-1-q-qz g(z)  $$
          <p>This equation can be solved numerically for a given real $z$ plus a tiny imaginary part. Then $\frac{1}{\pi}\text{Im}[g(z+i\epsilon)]$ gives us our desired spectral density. This classic case is also of course analytically tractable, with</p>
          $$g(z) = \frac{z-1-q\pm \sqrt{(\lambda_+-z)(z-\lambda_-) }}{2qz} $$
          <p>where $\lambda_\pm = (1\pm\sqrt(q))^2$. The discontinuity across the branch cut yields the Marchenko-Pastur distribution</p>

            $$\rho(z) =  \frac{\sqrt{(\lambda_+-z)(z-\lambda_-) }}{2\pi qz}+\mathbb{1}_{q > 1}(1-1/q)\delta(z)$$

          <p>I present the numerical, analytical, and empircal comparison in Fig. 1.  </p>
          <figure>
            <!-- put your image into the repo’s images folder -->
            <img src="figures/mp_q_2.png" alt="mp spec density">
            <figcaption>Figure 1. White wishart, Marchenko-Pastur law. The dashed curve is the numerical solution and the solid curve is the analytic expression.</figcaption>
          </figure>

          <p> For comparison I also present the fractional excess of the empirical density relative to the theory in Figure 2 below. The dispersion is symmetric as one would expect</p>

          <figure>
            <!-- put your image into the repo’s images folder -->
            <img src="figures/mp_frac_excess.png" alt="mp frac excess">
            <figcaption>Figure 2. The fractional excess of the empirical spectral density relative to the theoretical. The dispersion is symmetric.</figcaption>
          </figure>
  
          <p>For finite and generic $\alpha$, the $t$-integral in the self-consistent equation is not analytically tractable, but it can be handled numerically.</p>

          <h2>Numerical Implementation</h2>
          <p>For numerical implementation we need to be slightly more specific about what we mean by the distribution $dH(t)$.  The integral will not converge without a finite cutoff $\Lambda > 0$. In this case the distribution is</p>
            $$ dH(t) = \frac{1}{\alpha}\frac{\Lambda^{1/\alpha}}{1-\Lambda^{1/\alpha}}t^{-1-1/\alpha}$$
          <p>Additionally, in order to improve numerically stability, it is best to switch variables to the logarithm of $t$ and to divide both sides of the self-consistent equation by $g(z)$. This way, the numerical solver is not 
            trying to identify cancellations between large numbers, but reliably computes an order-one number when looking for a solution.  This form of the self-consistent equation is
          </p>
          $$ 1 = \frac{1}{\alpha}\frac{1}{1-\Lambda^{1/\alpha}} \int\limits_{\log(\Lambda)}^0 \frac{e^{t+t/\alpha} }{g(z)(\Lambda*\Delta+z e^{t}) }dt$$
          <p>First it is instructive to look at the rank-ordered plot of the eigenvalues of the sample covariance. This is what is often looked at the in the literature for natural datasets and it is instructive to keep in mind what it looks like for exactly power law data. This is presented in Fig. 3.
            Then I compare the numerical solution for the spectral density to empirics for $\alpha = 1.5$ and $q = 2$ in Fig. 4, finding solid agreement. </p>
          <figure>
            <!-- put your image into the repo’s images folder -->
            <img src="figures/pl_rank_ordered_q_2.png" alt="structured wishart">
            <figcaption>Figure 3. Rank-ordered eigenvalues for data with $\alpha = 1.5$ and $q = 2$.</figcaption>
          </figure>

          <p>There is a bit of a tail at the end, and by eye it is difficult to discern what sort of tail at the smaller eigenvalues is to be expected for samples from pure power-law data and what constitutes a systematic deviation. 
            It will be fruitful to examine the spectral density and the fractional empirical excesses in order to analyze this.
          </p>
          <figure>
            <!-- put your image into the repo’s images folder -->
            <img src="figures/pl_spec_density_q_2.png" alt="structured wishart">
            <figcaption>Figure 4. Spectral density for data with $\alpha = 1.5$ and $q = 2$.</figcaption>
          </figure>
          <p>The fractional empirical excess is presented in Fig. 5. Again, symmetric dispersion is found, indicative of the goodness of fit.
          </p>

          <figure>
            <!-- put your image into the repo’s images folder -->
            <img src="figures/pl_frac_excess_q_2.png" alt="structured wishart">
            <figcaption>Figure 5. Fractional empirical excess for data with $\alpha = 1.5$ and $q = 2$.</figcaption>
          </figure>

          <p>It is also interesting to consider equiparameterized samples i.e. samples with $q \sim 1$. Close to equiparameterization, there are two power-laws for two parts of the distribution, depicted for $\alpha = 1.7$ and $q = .99$ in Fig. 3. The larger eigenvalues fall as that population density dictates
            $\sim z^{-1-/\alpha}$ before transitioning to the $z^{-1/2}$ falloff of the density of smaller eigenvalues. The latter is also observed in the white Wishart case.
          The agreement is solid again, and again the dispersion is symmetric. We emphasize this point as we will see systematic deviations in the case of natural data which may be indicative of interesting phenomena. </p>

            <div style="display: flex; flex-wrap: wrap; gap: 1.5rem; align-items: flex-start;">
              <figure style="flex: 1 1 0; text-align: center;">
                <img src="figures/pl_spec_density_q_99.png" alt="structured wishart">
              <figcaption>Figure 6. Equiparameterized, structured wishart with $\alpha = 1.7$ and $q = .99$. The line at smaller eigenvalues has slope -1/2 and the line at larger eigenvalues has slope $-1-1/\alpha$ .</figcaption>
              </figure>
          
              <figure style="flex: 1 1 0; text-align: center;">
                <img src="figures/pl_frac_excess_q_99.png" alt="excess figure" style="max-width:100%;">
                <figcaption>Figure 7. Fractional excess of empirical distribution relative to theory. The dispersion is symmetric.</figcaption>
              </figure>
            </div>

         

         <h2>Natural Data: Images</h2>
         <h3>CIFAR10</h3>
         <p>We can now use our control over the theoretical spectral density for power-law population covariance and compare to natural data. We can start with the most naive comparison, which is to directly compare the sample covariance of the CIFAR-10 training set to 
          the power-law spectral density implied by its parameters. The first obvious issue with this is that the rank-ordered plot makes pretty manifest that there isn't an unambiguous power, and the falloff looks more like a geometric distribution, as I depict in Fig. 8.
         </p>   

         <figure>
          <!-- put your image into the repo’s images folder -->
          <img src="figures/cifar_rank_ordered.png" alt="white wishart">
          <figcaption>Figure 8. Rank-ordered, CIFAR-10 training set covariance eigenvalues.</figcaption>
        </figure>

        <p>There is a power-law stretch initially, but that tail is more than an artifact of finite over-parameterization, clearly. This fact is made completely manifest by the spectral density constructed from that power-law
          in Fig. 9. </p>
          <figure>
            <!-- put your image into the repo’s images folder -->
            <img src="figures/cifar_spec_density.png" alt="white wishart">
            <figcaption>Figure 9. Spectral density for CIFAR10 training set covariance eigenvalues with theory curve fitting the cutoff to agree empirically while using the exponent implied by the large eigenvalue behavior in the rank-ordered plot
              and the implied $q$ value from the input dimension and training set size.</figcaption>
          </figure>
          <p>The agreement is very poor for the raw CIFAR data, but as has been noted previously, the non-linear activations so central to neural networks, are capable of extending the power-law. This is demonstrated clearly by the plot of rank-ordered 
            eigenvalues, presented in Fig. 10.
          </p>
          <div style="display: flex; flex-wrap: wrap; gap: 1.5rem; align-items: flex-start;">
            <!-- put your image into the repo’s images folder -->
            <img src="figures/cifar_spec_density.png" alt="white wishart">
            <figcaption>Figure 10. Rank-ordered plot of eigenvalues for sample covariance in feature-mapped CIFAR.</figcaption>
          </figure>
        
            <figure style="flex: 1 1 0; text-align: center;">
              <img src="figures/cifar_relu_power_frac.png" alt="excess figure" style="max-width:100%;">
              <figcaption>Figure 11. Rank-ordered eigenvalues divided by the power-law. Some log-oscillatory behavior is present.</figcaption>
            </figure>
          </div>
            <p>The ReLU mapping clearly not only serves to extend beyond the trivial rank limitations, but drastically improves the agreement with a power-law spectral density. We see this in Fig. 12. The data has been feature-mapped up to 10k feature dimensions and then ReLU-mapped. 
            We therefore now use an overparameterization ratio of $q = \frac{1}{5}$ and a power-law $\alpha = 1.22$. These naive parameters agree extremely well.</p>
            <figure>
              <!-- put your image into the repo’s images folder -->
              <img src="figures/cifar_relu_spec_density.png" alt="white wishart">
              <figcaption>Figure 12. ReLU mapped CIFAR-10 data, mapped up to 10k feature dimensions. Theoretical spectral density for $\alpha = 1.22$ and $q = 1/5$.</figcaption>
            </figure>
            <p> Systematic deviations do remain at small eigenvalues, though.</p>

            <div style="display: flex; flex-wrap: wrap; gap: 1.5rem; align-items: flex-start;">
              <figure style="flex: 1 1 0; text-align: center;">
                <img src="figures/cifar_relu_spec_density_zoom.png" alt="structured wishart">
              <figcaption>Figure 13. Zoomed in version of Fig. 12, where a systematic discrepancy exists.</figcaption>
              </figure>
          
              <figure style="flex: 1 1 0; text-align: center;">
                <img src="figures/cifar_relu_frac_excess.png" alt="excess figure" style="max-width:100%;">
                <figcaption>Figure 14. The fractional empirical excess of the spectral density exhibits log-periodic oscillations at small eigenvalues..</figcaption>
              </figure>
            </div>
            <p>This excess is reminescent of the log-periodic oscillations observed around the rank-ordered plot of word frequencies around the Zipf's law prediction. The ReLU mapping improves the power-law and reveals two clear periods of oscillation in the spectral density with about period of about a decade. 
              Perhaps the phenomenon of log-periodic oscillations in the spectra of natural data around a power-law has a universality.
            </p>
            <h3>MNIST</h3>
            <p>The MNIST dataset is more amusing, as the training set of 60k images does have a full-rank covariance matrix in feature space. This is not a widely advertised fact, though 
              it has been noted elsewhere.  The raw input spectral density is in Fig. 10. We see the transition in the scaling of the empricical density characteristic of equiparameterization.
              The rank of the covariance in feature space can vary by a few depending on the finite precision, but is around 711.</p>
            <figure>
              <!-- put your image into the repo’s images folder -->
              <img src="figures/mnist_spec_density.png" alt="mnist">
              <figcaption>Figure 15. The spectrum of the MNIST training set. Despite a 60k images, the covariance is not full rank.</figcaption>
            </figure>
            <p>This near-equiparameteriation is mainted after feature mapping to 10k input dimensions and applying ReLU.</p>
            <div style="display: flex; flex-wrap: wrap; gap: 1.5rem; align-items: flex-start;">
              <figure style="flex: 1 1 0; text-align: center;">
                <img src="figures/mnist_relu_spec_density.png" alt="structured wishart">
              <figcaption>Figure 16.  The spectrum after random feature mapping and ReLU. The near equiparameterization is preserved.</figcaption>
              </figure>
          
              <figure style="flex: 1 1 0; text-align: center;">
                <img src="figures/mnist_relu_frac_excess.png" alt="excess figure" style="max-width:100%;">
                <figcaption>Figure 17. The fractional empircal excess is less well motivated for this poor fit, but in the region of better fit still exhibits some evidence of log periodic oscillation.</figcaption>
              </figure>
            </div>
            <p>Despite the inferior fit, I present the fractional empirical excess as well. This certainly not worth looking at in the small eigenvalue regime which violently disagrees as the density scales more flatly than $\lambda^{-1/2}$, but
              for the larger eigenvalues the dispersion does have some evidence of log-periodic oscillation that might survive a more rigorous analysis.
            </p>

            <h2>Natural Data: Text</h2>
            It's also interesting to look at natural language. Here the design matrix is contingent on both a tokenization scheme and a choice of embedding dimension. For tokenziation I use custom Byte Pair Encoding (BPE)
            from Hugging face trained on the first 5% of "wikitext-2-raw-v1". I use a vocab size of 8k, and use 100k tokens as the input data whose spectrum I study.  Despite the 100k tokens, there are only 7216 unique
            tokens, which is roughly consistent with the expectation from Heap's law. In particular, though, an embedding dimension above 7216 will furnish a rank-deficient design matrix. We look at embedding dimensions above and below this unique token number.
            After embedding and centering, we apply ReLU and compute the covariance from the ReLU-mapped design matrix. 


            <div style="display: flex; flex-wrap: wrap; gap: 1.5rem; align-items: flex-start;">
              <figure style="flex: 1 1 0; text-align: center;">
                <img src="figures/wiki_emb_8k_spec_density.png" alt="structured wishart">
              <figcaption>Figure 18. The spectral density uses a vocab of size 8k and then an embedding of dimension 8k. The effective $q$ taking the ratio of the embedding dimension and number of unique tokens is close to the theory curve with $q = 1.14$ and $\alpha = 1$.</figcaption>
              </figure>
          
              <figure style="flex: 1 1 0; text-align: center;">
                <img src="figures/wiki_8k_frac_excess.png" alt="excess figure" style="max-width:100%;">
                <figcaption>Figure 19. The empirical fractional excess exhibits some oscillatory behavior.</figcaption>
              </figure>
            </div>
              With $n_{\text{embed}} = 8k$ we have an implied $q \sim 1.1$. We produce the above theory curve with $q = 1.14$ and $\alpha = 1$.  We see quite a good fit, and again
              log-periodic structure to the excess density at smaller eigenvalues.  We can make a separate comparison with an embedding dimension $n_{\text{embed}} = 4k$.  In this case we get Fig. 15 and Fig. 16.
              
              <div style="display: flex; flex-wrap: wrap; gap: 1.5rem; align-items: flex-start;">
                <figure style="flex: 1 1 0; text-align: center;">
                  <img src="figures/wiki_emb_4k_spec_density.png" alt="structured wishart">
                <figcaption>Figure 20. Again, the appropriate $q$ is roughly the embedding dimension divided by the number of embedding dimensions. The theory curve has $q = .58$.</figcaption>
                </figure>
            
                <figure style="flex: 1 1 0; text-align: center;">
                  <img src="figures/wiki_4k_frac_excess.png" alt="excess figure" style="max-width:100%;">
                  <figcaption>Figure 21. Again, some oscillatory behavior is present in the fractional empirical excess.</figcaption>
                </figure>
              </div>
              In this case there is an implied overparameterization ratio $q \sim .55$. We use a fit with $q = .58$ and $\alpha = .98$. Again we see similar systematic deviations at small eigenvalues.
              <h2>Summary</h2>
              Getting enough control on the computation of the theoretical spectral density for power-law structured input data was sufficient to make some refined observations about natural data. The non-linear feature maps 
              not only extend the power-law behavior, as observed before, but indeed improve the fit to a power-law spectral density. It would be interesting to probe whether the log-periodic oscillatory structure is universal 
              and has any implications for training loss. 
              <h3>Acknowledgments</h3>
              <p>I would like to thank Alex Atanasov and Cegniz Pehlevan for valuable conversations related to the work herein.</p>
        </div>
      </article>
    </div>
  </body>