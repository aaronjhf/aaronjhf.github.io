<head>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">
  <meta name="author" content="Aaron Hillman">
  <link rel="stylesheet" href="/css/academic.min.4731b10c3a8aac85ab261b8ce101e025.css">
  
    <!-- 1) MathJax v3 config -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$','$'], ['\\(','\\)']],
          displayMath: [['$$','$$'], ['\\[','\\]']]
        }
      };
    </script>
  
    <!-- 2) MathJax v3 loader -->
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
      async>
    </script>
  
    <title>Power Law Spectral Densities and Natural Data | Aaron Hillman</title>
  </head>



  <body>
    <!-- copy over your <nav>…</nav> from index.html -->
  
    <div class="universal-wrapper pt-3">
      <article>
        <header class="article-header">
          <h1 class="article-title">Power Law Spectral Densities and Natural Data</h1>
          <div class="article-metadata">
            <span class="article-date">April 29, 2025</span>
          </div>
        </header>
  
        <div class="article-style">
          <h2>Background</h2>

          <p> It has been observed that the rank-ordered eigenvalues of the covariance matrices of natural datasets exhibit a power-law falloff. This is upstream of the scaling laws that have been observed for the loss curves of models versus model size and training set size.
            In this post I numerically calculate the spectral densities for power-law data and use the implementation to study natural data more closely.
            The case of Gaussian input data, yielding the famous Marchenko-Pastur law for its eigenvalue distribution, already provides the qualitative picture of double-descent and its mitigation by regularization, but does not exhibit power-law behavior. 
          </p>

          <!-- Side-by-side figs of e.g. MNIST power-law and with ReLU extending -->
  
          <p>It will be useful to recall the matrix resolvent, the $S$-transform, and their relationship to the spectral density that we are after.  The matrix resolvent of an $N \times N$ matrix $A$ is:</p>

          $$ \mathbf{G}_{\mathbf{A}}  = (z \mathbf{I}-\mathbf{A})^{-1}$$
          
          <p> The $S$-transform of the matrix $A$ is a scalar function obtained by taking the normalized trace </p>

          $$g_{\mathbf{A}}(z) = \frac{1}{N} \text{Tr}[\mathbf{G}_{\mathbf{A}}(z)] = \frac{1}{N}\sum\limits_k \frac{1}{\lambda_k-z}$$
          
          <p> This sum over the eigenvalues can of course be written as an integral over the spectral density:</p>

          $$g(z) = \int\limits_{-\infty}^\infty \frac{\rho(z)}{\lambda-z}d\lambda $$

          <p>This relationship, and its inversion, are crucial as there exists a self-consistent equation for $g(z)$ which we can leverage to determine the spectral density.
            The inversion of the above relationship is given by:
          </p>

          $$\lim_{\epsilon \to 0}\text{Im}[g(z+i\epsilon)] = \pi \rho(z) $$

          The matrices we want to apply this to are sample covariance matrices for input data with design matrix $X^{\alpha, i} \in \mathbb{R}^{P \times D}$ drawn from Gaussian with population covariance

          $$\Sigma_P = \text{diag}(1, \dots, \lambda_k, \dots \lambda_D) $$

          <p>with $\lambda_k \sim k^{-\alpha}$ for some non-negative exponent $\alpha$.  The classic Marchenko-Pastur case corresponds to $\alpha = 0$.  We want to compute the $S$-transform of the sample covariance
          </p>
          $$ \Sigma_S = \frac{X^T X}{P} $$
          <p>in the limit as $P, D \to \infty$ with $q = D/P$ held fixed. The theory curve for sample covariances in the large dimension limit are what we will compare to the empirical
            sample covariances for natural data, such computer vision datasets like CIFAR-10 and MNIST, or natural language datasets like Wikitext.
          </p>

          <p> The final ingredient needed is the most non-trivial: the self-consistent equation for $g(z)$. This is the Silverstein equation: </p>

          $$ g(z) = \int \frac{1}{t \Delta +z}dH(t) $$

          <p>where $\Delta$ is</p>
          
          $$\Delta = -1+q-qzg(z) $$

          <p> and $dH(t)$ is the density of the eigenvalues of the population covariance matrix, $dH(t) \sim t^{-1-1/\alpha}$  for $\alpha > 0$ and $\delta(t-1)$ when $\alpha =0$.
            Indeed we recover the usual equation for $g(z)$ for the $\alpha = 0$ case
          </p>
          $$ \frac{1}{g(z)} = z-1-q-qz g(z)  $$
          <p>This equation can be solved numerically, for $z$ adding a tiny imaginary part. Then $\frac{1}{\pi}\text{Im}[g(z+i\epsilon)]$ gives us our desired spectral density. This classic case is also of course analytically tractable, with</p>
          $$g(z) = \frac{z-1-q\pm \sqrt{(\lambda_+-z)(z-\lambda_-) }}{2qz} $$
          <p>with $\lambda_\pm = (1\pm\sqrt(q))^2$. The discontinuity across the branch cut yielding the Marchenko-Pastur distribution</p>

            $$\rho(z) =  \frac{\sqrt{(\lambda_+-z)(z-\lambda_-) }}{2\pi qz}$$

          <p>We present the numerical, analytical, and empircal comparison in Fig. 1.  </p>
          <figure>
            <!-- put your image into the repo’s images folder -->
            <img src="figures/wishart_white_q1_3.png" alt="white wishart">
            <figcaption>Figure 1. White wishart, Marchenko-Pastur.</figcaption>
          </figure>
  
          <p>For finite and generic $\alpha$, the $t$-integral in the self-consistent equation is not analytically tractable, but it can be handled numerically.</p>

          <h2>Numerical Implementation</h2>
          <p>For numerical implementation we need to be slightly more specific about what we mean by the distribution $dH(t)$.  The integral will not converge without a finite cutoff $\Lambda$. So the distribution </p>
            $$ dH(t) = \frac{1}{\alpha}\frac{\Lambda^{1/\alpha}}{1-\Lambda^{1/\alpha}}t^{-1-1/\alpha}$$
          <p>Additionally, in order to improve numerically stability, it is best to use logarithmic variables $\log(t)$ and to divide both sides of the self-consistent equation by $g(z)$. This way, the numerical solver is not 
            trying to identify cancellations between large numbers, but reliably computes an order-one number looking for a solution.  This form of the self-consistent equation is
          </p>
          $$ 1 = \frac{1}{\alpha}\frac{1}{1-\Lambda^{1/\alpha}} \int\limits_{\log(\Lambda)}^0 \frac{e^{t+t/\alpha} }{m_F(\Lambda*\Delta+z e^{t}) }dt$$
        </div>
      </article>
    </div>
  </body>